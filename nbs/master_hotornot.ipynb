{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications import ResNet50\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from keras.utils.data_utils import get_file\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image , ImageFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop face with opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cropping faces\n",
    "in_folder = \"./SCUT-FBP5500_v2/Images/\"\n",
    "out_folder = \"./SCUT-FBP5500_v2/cropped_Images_close/\"\n",
    "\n",
    "onlyfiles = [f for f in os.listdir(in_folder) if os.path.isfile(os.path.join(in_folder, f))]\n",
    "face_classifier = cv2.CascadeClassifier('Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "for file in onlyfiles:\n",
    "    \n",
    "    image = cv2.imread(in_folder+file)\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        \n",
    "      print(\"No faces found\")\n",
    "      print(file)\n",
    "      cv2.imwrite(out_folder+file,image)\n",
    "\n",
    "    else:\n",
    "        for (x,y,w,h) in faces:\n",
    "#             x_start = y - bounding_offset  if y - bounding_offset >= 0  else y\n",
    "#             x_end = y+h+bounding_offset  if y+h+bounding_offset <= image.shape[0]  else y+h\n",
    "#             y_start =  x - bounding_offset if x - bounding_offset >= 0 else x\n",
    "#             y_end = x+w+bounding_offset  if x+w+bounding_offset <= image.shape[1] else x+w\n",
    "            cropped_img = image[x:x+w, y:y + h, :]    \n",
    "#             cropped_img = image[y - bounding_offset:y+h+bounding_offset,x - bounding_offset:x+w+bounding_offset,:]\n",
    "            cv2.imwrite(out_folder+file,cropped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation\n",
    "import random \n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "def blur_at_random(img_array):    \n",
    "    img =  Image.fromarray(img_array.astype('uint8'))\n",
    "    blurred_image = img.filter(ImageFilter.BoxBlur(random.randint(1,4)))\n",
    "    result = np.array(blurred_image)\n",
    "    return result \n",
    "\n",
    "def generate_img(filename):\n",
    "    img = load_img(in_folder + filename)    \n",
    "    img.save(out_folder+filename)\n",
    "    x = img_to_array(img)\n",
    "    x = x.reshape((1,) + x.shape)\n",
    "    i = 0\n",
    "    for batch in datagen.flow(x, batch_size=1,\n",
    "                              save_to_dir=out_folder, \n",
    "                              save_prefix=filename.split(\".\")[0],\n",
    "                              save_format='jpg'):\n",
    "        break\n",
    "\n",
    "in_folder = \"./SCUT-FBP5500_v2/cropped_Images/\"\n",
    "out_folder = \"./SCUT-FBP5500_v2/augmented/\"\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=blur_at_random,\n",
    "    rotation_range=20,\n",
    "    channel_shift_range = 50,\n",
    "    horizontal_flip=True\n",
    "    )\n",
    "\n",
    "onlyfiles = [f for f in os.listdir(in_folder) if os.path.isfile(os.path.join(in_folder, f))]\n",
    "\n",
    "\n",
    "for file in onlyfiles:\n",
    "    generate_img(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading training images into memory \n",
    "\n",
    "\n",
    "folder = \"./SCUT-FBP5500_v2/augmented/\"\n",
    "onlyfiles = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "train_files = []\n",
    "y_train = []\n",
    "\n",
    "i=0\n",
    "\n",
    "for _file in onlyfiles:\n",
    "    train_files.append(_file)   \n",
    "    y_train.append(_file.title())\n",
    "\n",
    "print(\"Files in train_files: %d\" % len(train_files))\n",
    "\n",
    "image_width = 350\n",
    "image_height = 350\n",
    "channels = 3\n",
    "\n",
    "\n",
    "dataset = np.ndarray(shape=(len(train_files), image_width, image_height,  channels),\n",
    "                     dtype=np.float32)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for _file in train_files:\n",
    "    img = load_img(folder + \"/\" + _file,target_size=(350, 350))  # this is a PIL image\n",
    "    #img = load_img(folder + \"/\" + _file,target_size=(224, 224))\n",
    "    x = img_to_array(img)  \n",
    "    x = x  / 255.0\n",
    "    dataset[i] = x\n",
    "    i = i + 1\n",
    "    if i % 250 == 0:\n",
    "         print(\"%d images to array\" % i)\n",
    "\n",
    "print(\"All images to array!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare train_y (rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"./SCUT-FBP5500_v2/All_Ratings.xlsx\")\n",
    "df = df.groupby([\"Filename\"])[\"Rating\"].mean()\n",
    "df = df.to_frame()\n",
    "df.to_csv('ratings.csv')\n",
    "df = pd.read_csv('ratings.csv',index_col='Filename')\n",
    "\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    transformed = y_train[i].split(\".\")\n",
    "    transformed[0] = transformed[0].upper()\n",
    "    transformed[0] = transformed[0].split(\"_\")[0]\n",
    "    transformed[1] = transformed[1].lower()\n",
    "    transformed = \".\".join(transformed)\n",
    "    y_train[i] = transformed\n",
    "\n",
    "y_train_label = []\n",
    "for file_name in y_train:\n",
    "    target_row = df.loc[file_name]\n",
    "    y_train_label.append(target_row['Rating'])\n",
    "    \n",
    "    \n",
    "y_train_label =  np.array(y_train_label)\n",
    "y_train_label = y_train_label.reshape((16500,1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet50(include_top=False, pooling='avg')\n",
    "model = Sequential()\n",
    "model.add(resnet)\n",
    "model.add(Dense(1))\n",
    "model.layers[0].trainable = False\n",
    "print(model.summary()) \n",
    "model.compile(loss='mean_squared_error', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(batch_size=32, x=dataset, y=y_train_label, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
